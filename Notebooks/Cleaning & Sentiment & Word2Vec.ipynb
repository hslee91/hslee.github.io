{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17354\n",
      "17354\n",
      "17354\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/Henna/Desktop/Scraping/Data/dataset_gur_iac.csv', encoding='ISO-8859-1')\n",
    "print(len(df))\n",
    "df.head()\n",
    "\n",
    "text = df[\"text\"]\n",
    "y = df[\"resistance attempt\"]\n",
    "\n",
    "print(len(text))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>resistance attempt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9123</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The guy is a sleep scientist and yet he makes ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "9123         NaN  The guy is a sleep scientist and yet he makes ...   \n",
       "\n",
       "      resistance attempt  \n",
       "9123                   1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_rows = df[df.isnull().T.any().T]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(str(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_polarity(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def sentiment_subjectivity(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment.subjectivity\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['polarity'] = df['text'].apply(sentiment_polarity)\n",
    "df['subjectivity'] = df['text'].apply(sentiment_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>resistance attempt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7719</td>\n",
       "      <td>Well yes.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7720</td>\n",
       "      <td>Exact, to the point, &amp; beautiful.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7721</td>\n",
       "      <td>Yes, but it also has consequences.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7722</td>\n",
       "      <td>k</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7723</td>\n",
       "      <td>And its all evolved via NDS - marvellous!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                       text  resistance attempt\n",
       "0       7719                                  Well yes.                 0.0\n",
       "1       7720          Exact, to the point, & beautiful.                 0.0\n",
       "2       7721         Yes, but it also has consequences.                 0.0\n",
       "3       7722                                          k                 0.0\n",
       "4       7723  And its all evolved via NDS - marvellous!                 0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'exact', u'point', u'beauti']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your chose\n",
    "def clean_text(text, ):\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "    def remove_special_characters(text, characters=string.punctuation.replace('-', '')):\n",
    "        tokens = tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(characters)))\n",
    "        return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "    def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "    def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    text = text.strip(' ') #strip whitespaces\n",
    "    text = text.lower() #lowercase\n",
    "    text = stem_text(text) #stemming\n",
    "    text = remove_special_characters(text) #remove punctuation and symbols\n",
    "    text = remove_stopwords(text) #remove stopwords\n",
    "    #text.strip(' ') # strip white spaces again?\n",
    "\n",
    "    return text\n",
    "\n",
    "text = text.tolist()\n",
    "cleaned_text = [clean_text(i) for i in text]\n",
    "print(len(cleaned_text))\n",
    "\n",
    "#tokenizing\n",
    "tokenized_text = [word_tokenize(i) for i in cleaned_text]\n",
    "tokenized_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-72ba2a5a794f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel_fasttext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fasttext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigrams = Phrases(cleaned_text)\n",
    "\n",
    "model_fasttext = FastText(cleaned_text=bigrams[sentences], size=300, window=5, min_count=5, workers=4, sg=1)\n",
    "\n",
    "print(model_fasttext)\n",
    "model_fasttext.wv.most_similar(\"gove\")\n",
    "\n",
    "#saving\n",
    "#model_fasttext.save(\"w2v_model_fasttext_d300\")\n",
    "#model = Word2Vec.load(\"w2v_model_fasttext_d300\")\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc\n",
    "\n",
    "def filter_docs(corpus, texts, labels, polarity, subjectivity, condition_on_doc):\n",
    "    \"\"\"\n",
    "    Filter corpus, texts and labels given the function condition_on_doc which takes\n",
    "    a doc.\n",
    "    The document doc is kept if condition_on_doc(doc) is true.\n",
    "    \"\"\"\n",
    "    number_of_docs = len(corpus)\n",
    "\n",
    "    if texts is not None:\n",
    "        texts = [text for (text, doc) in zip(texts, corpus)\n",
    "                 if condition_on_doc(doc)]\n",
    "        \n",
    "    polarity = [i for (i, doc) in zip(polarity, corpus) if condition_on_doc(doc)]\n",
    "    subjectivity = [i for (i, doc) in zip(subjectivity, corpus) if condition_on_doc(doc)]\n",
    "    labels = [i for (i, doc) in zip(labels, corpus) if condition_on_doc(doc)]\n",
    "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
    "\n",
    "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
    "\n",
    "    return (corpus, texts, labels, polarity, subjectivity)\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.vocab]\n",
    "    return np.mean(word2vec_model[doc], axis=0)\n",
    "\n",
    "def has_vector_representation(word2vec_model, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in word2vec_model.wv.vocab for word in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the functions\n",
    "corpus = []\n",
    "corpus = [preprocess(text) for text in clean_text]\n",
    "\n",
    "corpus, texts, y, polarity, subjectivity = filter_docs(corpus, clean_text, y, polarity, subjectivity, lambda doc: (len(doc) != 0))\n",
    "\n",
    "corpus, texts, y, polarity, subjectivity = filter_docs(corpus, clean_text, y, polarity, subjectivity, lambda doc: has_vector_representation(model_fasttext, doc))\n",
    "\n",
    "print(len(corpus))\n",
    "print(len(texts))\n",
    "print(len(y))\n",
    "print(len(polarity))\n",
    "print(len(subjectivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making to numpy array\n",
    "x =[]\n",
    "for doc in corpus: #look up each doc in model\n",
    "    x.append(document_vector(model_fasttext, doc))\n",
    "    \n",
    "X = np.array(x) #list to array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
